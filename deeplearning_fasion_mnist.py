# -*- coding: utf-8 -*-
"""DeepLearning_Fasion_Mnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vFz2NzYOmCLc09x2LZVbpwuPio1Wgdqg

**Objective**:
1. Select and preprocess an image dataset.
2. Implement and train an Auto-Encoder (AE) on your dataset.
3. Implement and train a Variational Auto-Encoder (VAE) on your dataset.
4. Experiment with various tuning parameters and analyze their effects on model performance.
5. Compare and contrast the performance and characteristics of AEs and VAEs

**Importing the necessary libraries**
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Dense, Flatten, Reshape
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
import tensorflow_hub as hub
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow import keras

"""**1. Dataset Selection and Preprocessing**

**Loading the DATASET**

Here I'm using the Fashion Mnist dataset from keras datasets.
"""

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

"""**Data Pre-Processing:**

1. Resized Images: Resized each image in the dataset to 64x64 pixels using tf.image.resize.
2. Normalized Pixel Values: Scaled the pixel values of each image to be between 0 and 1 by dividing by 255.0.
3. Splited Dataset: Splited the dataset into training and validation sets using the specified percentages.
4. Reshaped the Data: Reshaped the data such that it could be given as input to Auto encoder and variational auto encoder models.
"""

train_images = train_images / 255.0

test_images = test_images / 255.0

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

# Split the dataset into training and validation sets
train_images, val_images, train_labels, val_labels = train_test_split(
    train_images, train_labels, test_size=0.999, random_state=42
)

#Reshaping data for CNN
train_images = train_images.reshape(-1, 28, 28, 1)
val_images = val_images.reshape(-1, 28, 28, 1)

"""**2. Auto-Encoder (AE) Implementation**

**Model Architecture:**

### Encoder Structure:
1. **Input Layer:**
   - Shape: (28, 28, 1) indicating grayscale images with dimensions 28x28 pixels.
   
2. **Encoded Layer 1:**
   - Convolutional Layer: 64 filters, kernel size (3, 3), activation function ReLU, padding 'same'.
   - MaxPooling2D Layer: Pooling size (2, 2), padding 'same'.

3. **Encoded Layer 2:**
   - Convolutional Layer: 32 filters, kernel size (3, 3), activation function ReLU, padding 'same'.
   - MaxPooling2D Layer: Pooling size (2, 2), padding 'same'.

4. **Encoded Layer 3:**
   - Convolutional Layer: 16 filters, kernel size (3, 3), activation function ReLU, padding 'same'.

5. **Latent View Layer:**
   - MaxPooling2D Layer: Pooling size (2, 2), padding 'same'.
   - This layer captures the reduced-dimensional representation (latent view) of the input.

### Decoder Structure:
1. **Decoded Layer 1:**
   - Convolutional Layer: 16 filters, kernel size (3, 3), activation function ReLU, padding 'same'.
   - UpSampling2D Layer: Upsampling factor (2, 2).

2. **Decoded Layer 2:**
   - Convolutional Layer: 32 filters, kernel size (3, 3), activation function ReLU, padding 'same'.
   - UpSampling2D Layer: Upsampling factor (2, 2).

3. **Decoded Layer 3:**
   - Convolutional Layer: 64 filters, kernel size (3, 3), activation function ReLU.
   - UpSampling2D Layer: Upsampling factor (2, 2).

4. **Output Layer:**
   - Convolutional Layer: 1 filter, kernel size (3, 3), padding 'same'.
   - The final layer reconstructs the denoised image with the same dimensions as the input.
"""

# input layer
input_layer = Input(shape=(28, 28, 1))

# Encoding Architecture
encoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)
encoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)

encoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)
encoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)

encoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)

latent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)

# decoding architecture
decoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)
decoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)

decoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)
decoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)

decoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)
decoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)


output_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)

#dimension of output layer == input layer coz we need denoised image thats why there is no dense layer only Conv layer

"""**Learning Rate and Optimizer:** The original model is trained with the default learning rate for the Adam optimizer.

**Loss Function:** The model was trained using Mean Squared Error (MSE) as the loss function.
"""

# compile the model
model = Model(input_layer, output_layer)
model.compile(optimizer='adam', loss='mse')

model.summary()

"""**Training Process**

Dataset:
The model is trained on the Fashion MNIST dataset, which consists of grayscale images of fashion items.

Data Preprocessing:
The pixel values of the images are normalized to the range [0, 1] by dividing by 255.

Training Parameters:
The model is trained for 10 epochs with a batch size of 2048.
"""

history = model.fit(train_images, train_labels, epochs=25, batch_size=2048)

"""**Performance**: After 25 epochs, the model shows a gradual decrease in the loss, reaching approximately 10.1654. However, the reduction in loss slows down towards the later epochs.


This model does not explicitly incorporate regularization techniques such as dropout or L2 regularization. Regularization is a common practice to prevent overfitting, but its absence suggests that the model architecture and dataset may not require additional regularization measures for this specific task.
In summary, the original model for denoising grayscale images is trained using the Adam optimizer and Mean Squared Error as the loss function. It undergoes 25 epochs of training on the Fashion MNIST dataset, with the training loss monitored at each epoch. Regularization techniques are not explicitly applied, indicating that the model is designed to balance complexity without overfitting on the given dataset.

**Hyperparameter Tuning**

In the modified model, several changes were introduced, including adjustments to the number of layers in the encoding and decoding architectures, reduction in the latent space dimensionality, and the addition of the learning rate as a hyperparameter for the Adam optimizer.

**Changes in Encoding and Decoding Architectures:**

**Reduction in Encoding Layers:**

Decreasing the number of encoding layers from 3 to 2 (convolutional and pooling layers).

**Reduction in Decoding Layers:**

Similarly, reducing the number of decoding layers may limit the model's ability to reconstruct fine details.

**Reduction in Latent Space Dimensionality:**

The reduction in the latent space dimensionality from 16 to 8.

**Learning Rate and Optimizer:** The new model was trained with a learning rate of 0.001 for the Adam optimizer.

Adding the learning rate as a hyperparameter for the Adam optimizer provides more control over the training process. A well-tuned learning rate can influence the convergence speed and stability of the training. A too high or too low learning rate might hinder convergence or result in oscillations, affecting the reconstruction quality.

**Loss Function:** Similar to the original model, the new model also utilized Mean Squared Error (MSE) as the loss function.
"""

# Encoding Architecture
encoded_layer1_new = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
encoded_layer1_new = MaxPool2D((2, 2), padding='same')(encoded_layer1)

encoded_layer2_new = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer1_new)
encoded_layer2_new = MaxPool2D((2, 2), padding='same')(encoded_layer2)

latent_view_new = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded_layer2_new)
latent_view_new = MaxPool2D((2, 2), padding='same')(latent_view_new)

# Decoding architecture
decoded_layer1_new = Conv2D(8, (3, 3), activation='relu', padding='same')(latent_view_new)
decoded_layer1_new = UpSampling2D((2, 2))(decoded_layer1_new)

decoded_layer2_new = Conv2D(16, (3, 3), activation='relu', padding='same')(decoded_layer1_new)
decoded_layer2_new = UpSampling2D((2, 2))(decoded_layer2_new)

decoded_layer3_new = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer2_new)
decoded_layer3_new = UpSampling2D((2, 2))(decoded_layer3_new)

output_layer_new = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded_layer3_new)

# Compile the model
modelnew = Model(input_layer, output_layer)
modelnew.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

modelnew.summary()

history2 = modelnew.fit(train_images, train_labels, epochs=25, batch_size=2048)

"""**Impact on Reconstruction Quality and Training Dynamics:**

The reduction in the number of layers and latent space dimensionality may simplify the model but could potentially lead to a trade-off between simplicity and the ability to capture intricate details in the reconstruction.
The learning rate can significantly influence the training dynamics. A carefully chosen learning rate can facilitate quicker convergence and more stable training. However, an inappropriate learning rate might cause the model to converge too quickly, resulting in suboptimal solutions, or converge too slowly, delaying the learning process.
"""

# Plotting the training history for the original model
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Original Model')
plt.title('Original Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plotting the training history for the new model
plt.subplot(1, 2, 2)
plt.plot(history2.history['loss'], label='New Model')
plt.title('New Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""**Comparison Observation:**

**Learning Rate and Optimizer:** Both models used the Adam optimizer, but the new model incorporated a specific learning rate (0.001). This adjustment might have contributed to the more efficient learning observed in the new model.

**Loss Function:** Both models employed Mean Squared Error as the loss function, ensuring a consistent metric for comparison.

**Performance:**The new model outperforms the original model in terms of the final training loss after 25 epochs. The modifications made to the architecture appear to have a positive impact on the model's ability to capture essential features in the data.

In summary, the new model, with a lower learning rate and architectural adjustments, achieves a lower final training loss after 25 epochs compared to the original model. This suggests that the changes made to the model have positively influenced its learning capabilities.

**3. Variational Auto-Encoder (VAE) Implementation**

**Model Architecture**

### VAE (vae):

#### Encoder:
- **Structure:**
  - Convolutional Layer (Conv2D) with ReLU activation and batch normalization.
  - Convolutional Layer (Conv2D) with ReLU activation and batch normalization.
  - Flatten layer.
  - Dense Layer with ReLU activation.
  - Dense Layer with linear activation for mean.
  - Dense Layer with linear activation for log variance.

- **Latent Dimension:** 2

#### Decoder:
- **Structure:**
  - Dense Layer with ReLU activation.
  - Dense Layer with ReLU activation.
  - Reshape layer.
  - Convolutional Transpose Layer (Conv2DTranspose) with ReLU activation and batch normalization.
  - Convolutional Transpose Layer (Conv2DTranspose) with sigmoid activation.

- **Latent Dimension:** 2

**Learning Rate and Optimizer:** The original model is trained with the default learning rate for the Adam optimizer.

**Loss Function:** The loss function in a VAE typically consists of two components.

**a. Reconstruction Loss**: Measures the difference between the input and the reconstructed output. Commonly, it is the binary cross-entropy loss for image data or mean squared error (MSE) for continuous data.

**b. KL Divergence Loss:** Measures the difference between the learned latent distribution and the prior distribution. It encourages the latent space to follow a specific structure (e.g., unit Gaussian).



**Hyper Parameter Tuning**



### VAE1 (vae1):

#### Encoder1:
- **Structure:**
  - Convolutional Layer (Conv2D) with ReLU activation and batch normalization.
  - Convolutional Layer (Conv2D) with ReLU activation and batch normalization.
  - Flatten layer.
  - Dense Layer with ReLU activation.
  - Dense Layer with linear activation for mean.
  - Dense Layer with linear activation for log variance.

- **Latent Dimension:** 5

#### Decoder1:
- **Structure:**
  - Dense Layer with ReLU activation.
  - Dense Layer with ReLU activation.
  - Reshape layer.
  - Convolutional Transpose Layer (Conv2DTranspose) with ReLU activation and batch normalization.
  - Convolutional Transpose Layer (Conv2DTranspose) with sigmoid activation.

- **Latent Dimension:** 5

**Learning Rate:** The learning rate for Adam optimizer is explicitly set to 0.001.

**Optimzer:** Adam optimizer is used with a learning rate of 0.001.

**Loss Function:** The loss function is specified as Mean Squared Error (MSE).
"""

# Delete all previous models to free memory
tf.keras.backend.clear_session()

num_channels = 1

class Sampling(layers.Layer):
  """Uses (z_mean, z_log_var) to sample z, the vector encoding an image."""

  def call(self, inputs):
    z_mean, z_log_var = inputs
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

from keras import backend as K

#ENCODER
latent_dim = 2

encoder_inputs = keras.Input(shape=(28, 28, num_channels))
x = layers.Conv2D(32, 3, activation="relu", strides=2, padding="same")(encoder_inputs)
x = layers.Conv2D(64, 3, activation="relu", strides=2, padding="same")(x)

conv_shape = K.int_shape(x)  #shape of conv to be provided to decoder

x = layers.Flatten()(x)
x = layers.Dense(16, activation="relu")(x)
z_mean = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z = Sampling()([z_mean, z_log_var])
encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")
encoder.summary()

#ENCODER 2
latent_dim2 = 5

encoder_inputs = keras.Input(shape=(28, 28, num_channels))
x = layers.Conv2D(32, 3, activation="relu", strides=2, padding="same")(encoder_inputs)
x = layers.Conv2D(64, 3, activation="relu", strides=2, padding="same")(x)

conv_shape2 = K.int_shape(x)  #shape of conv to be provided to decoder

x = layers.Flatten()(x)
x = layers.Dense(16, activation="relu")(x)
z_mean = layers.Dense(latent_dim2, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim2, name="z_log_var")(x)
z = Sampling()([z_mean, z_log_var])
encoder2 = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder2")
encoder2.summary()

#DECODER
latent_inputs = keras.Input(shape=(latent_dim,))
x = layers.Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation="relu")(latent_inputs)
x = layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)
x = layers.Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same")(x)
x = layers.Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same")(x)
decoder_outputs = layers.Conv2DTranspose(num_channels, 3, activation="sigmoid", padding="same")(x)
decoder = keras.Model(latent_inputs, decoder_outputs, name="decoder")
decoder.summary()

#DECODER 2

latent_inputs2 = keras.Input(shape=(latent_dim2,))
x = layers.Dense(conv_shape2[1] * conv_shape2[2] * conv_shape2[3], activation="relu")(latent_inputs2)
x = layers.Reshape((conv_shape2[1], conv_shape2[2], conv_shape2[3]))(x)
x = layers.Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same")(x)
x = layers.Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same")(x)
decoder_outputs = layers.Conv2DTranspose(num_channels, 3, activation="sigmoid", padding="same")(x)
decoder2 = keras.Model(latent_inputs2, decoder_outputs, name="decoder2")
decoder2.summary()

class VAE(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.total_loss_tracker = keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = keras.metrics.Mean(
            name="reconstruction_loss"
        )
        self.kl_loss_tracker = keras.metrics.Mean(name="kl_loss")

    @property
    def metrics(self):
        return [
            self.total_loss_tracker,
            self.reconstruction_loss_tracker,
            self.kl_loss_tracker,
        ]


    def call(self, inputs, training=False):
        z_mean, z_log_var, z = self.encoder(inputs, training=training)
        reconstruction = self.decoder(z, training=training)
        return reconstruction

    def train_step(self, data):
        x,y = data
        #Use gradientTape to record everything we need to compute the gradient
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(x)
            reconstruction = self.decoder(z)
            if(x.shape[1]==28):
              reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(
                    keras.losses.binary_crossentropy(x, reconstruction), axis=(1, 2)
                )
            )
            if(x.shape[1]==784):
              reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(
                    keras.losses.binary_crossentropy(x, reconstruction), axis=None
                )
            )
            #reconstruction_loss = input_dim*keras.losses.binary_crossentropy(x, reconstruction)    #input_dim=784
            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
            total_loss = reconstruction_loss + kl_loss

        #Compute gradients
        grads = tape.gradient(total_loss, self.trainable_weights)
        #Apply gradients using the optimizer
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        #Update metrics
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
        }

    def test_step(self,data):
      x,y=data
      #Get the data
      z_mean, z_log_var, z = self.encoder(x,training=False)
      #Prediction
      reconstruction = self.decoder(z,training=False)
      #Loss
      if(x.shape[1]==28):
        reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(
                    keras.losses.binary_crossentropy(x, reconstruction), axis=(1, 2)
                )
            )
      if(x.shape[1]==784):
        reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(
                    keras.losses.binary_crossentropy(x, reconstruction), axis=None
                )
            )
      #reconstruction_loss = input_dim*keras.losses.binary_crossentropy(x, reconstruction)    #input_dim=784
      kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))
      kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
      total_loss = reconstruction_loss + kl_loss

      #Update metrics
      self.total_loss_tracker.update_state(total_loss)
      self.reconstruction_loss_tracker.update_state(reconstruction_loss)
      self.kl_loss_tracker.update_state(kl_loss)
      return {
          "loss": self.total_loss_tracker.result(),
          "reconstruction_loss": self.reconstruction_loss_tracker.result(),
          "kl_loss": self.kl_loss_tracker.result(),
      }

(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()
fashion_mnist_digits = np.concatenate([x_train, x_test], axis=0)

# Display images of the first 10 images in the training set and their true lables
fig, axs = plt.subplots(2, 5, sharey=False, tight_layout=True, figsize=(12,6), facecolor='white')
n=0
for i in range(0,2):
    for j in range(0,5):
        axs[i,j].matshow(x_train[n])
        axs[i,j].set(title=y_train[n])
        n=n+1
plt.show()

fashion_mnist_digits = np.expand_dims(fashion_mnist_digits, -1).astype("float32") / 255
x_train = np.expand_dims(x_train, -1).astype("float32") / 255
x_val = np.expand_dims(x_test, -1).astype("float32") / 255

#VAE 1
vae = VAE(encoder, decoder)
vae.compile(optimizer=keras.optimizers.Adam())
#history = vae.fit(mnist_digits, validation_split=0.3, epochs=20, batch_size=128)
history = vae.fit(x = x_train, y = x_train, validation_data=(x_val,x_val), epochs=5, batch_size=1024)

#VAE 2
vae2 = VAE(encoder2, decoder2)
vae2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='mse')
history2 = vae2.fit(x = x_train, y = x_train, validation_data=(x_val,x_val), epochs=5, batch_size=1024)

#VAE 1
# list all data in history
print(history.history.keys())
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#VAE2
# list all data in history
print(history2.history.keys())
# summarize history for loss
plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""The training outputs for the two VAE models (vae1 and vae2) are showing the evolution of the loss, reconstruction loss, and KL divergence across epochs.

**vae1 Model:**
- The training loss starts at 320.3403 and gradually decreases over epochs.
- The reconstruction loss (related to the fidelity of the generated samples) starts at 307.0434 and decreases over epochs.
- The KL divergence loss (related to the regularization of the latent space) starts at 9.1693 and also decreases over epochs.

**vae2 Model:**
- The training loss starts at 275.8260 and decreases over epochs.
- The reconstruction loss starts at 256.4097 and decreases over epochs.
- The KL divergence loss starts at 9.7829 and decreases over epochs.

**Comparison and Discussion:**
- Both models show a decrease in the overall loss, which indicates that they are learning and improving over epochs.
- vae2 has a lower starting loss compared to vae1, suggesting that vae2 might have started with a more favorable configuration or architecture.
- The reconstruction loss for vae2 is lower than that of vae1, indicating that vae2 is doing a better job of reconstructing the input samples.
- The KL divergence loss for vae2 is slightly higher than that of vae1, but both models show a decrease over epochs, indicating effective regularization.

### Summary:
- Both VAE and VAE1 have similar encoder and decoder structures, with the only difference being the latent dimension.
- The encoder consists of convolutional layers followed by flattening and dense layers for mean and log variance.
- The decoder mirrors the encoder with dense layers, reshape layer, and convolutional transpose layers.
- VAE has a latent dimension of 2, while VAE1 has a latent dimension of 5.
- Both VAE and VAE1 use the Adam optimizer, but VAE1 explicitly sets the learning rate to 0.001.
- The loss functions differ; VAE typically uses a combination of reconstruction loss and KL divergence loss, while VAE1 uses MSE for reconstruction.

**4. Analysis and Comparison**

**Reconstruction Quality**
"""

import matplotlib.pyplot as plt
import numpy as np

# Function to plot original and reconstructed images
def plot_images(original, reconstructed, title):
    n = 10  # Number of images to display
    plt.figure(figsize=(2 * n, 4))

    for i in range(n):
        # Display original images
        ax = plt.subplot(2, n, i + 1)
        plt.imshow(original[i].reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

        # Display reconstructed images
        ax = plt.subplot(2, n, i + 1 + n)
        plt.imshow(reconstructed[i].reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

    plt.suptitle(title)
    plt.show()

# Select random images from the dataset
num_images = 10
original_images = x_train[:num_images]

# Reconstruct images using the AE and VAE models
ae_reconstructed_images = model.predict(original_images)
vae_reconstructed_images = vae.predict(original_images)

# Plot side-by-side comparisons
plot_images(original_images, ae_reconstructed_images, title='Original vs AE Reconstruction')
plot_images(original_images, vae_reconstructed_images, title='Original vs VAE Reconstruction')

# Reconstruct images using the AE and VAE hypertuned models
ae1_reconstructed_images = model.predict(original_images)
vae1_reconstructed_images = vae2.predict(original_images)

# Plot side-by-side comparisons
plot_images(original_images, ae1_reconstructed_images, title='Original vs AE Hypertuned Reconstruction')
plot_images(original_images, vae1_reconstructed_images, title='Original vs VAE Hypertuned Reconstruction')

"""As we can infer from the above reconstructed images, it's clear that only vae model was able to reconstruct the images and ae models before and after hyper tuning failed to reconstruct the images, with ae models output we can barely recognice the images from the dataset. In pur case, VAE2 Reconstructed images outperforms VAE1 and AE Reconstructed images as you can see the reconstucted images in vae2 model are slightly better than the vae1 and ae models. Only VAE2 Reconstructed images can be used with unseen images to an extent and the other models reconstructed images cannot be used.

**Latent Space Analysis**
"""

num_images_ae = 10
original_ae = train_images[:num_images_ae]


# Extract the encoder part of the original model
encoder_model = Model(inputs=model.input, outputs=model.get_layer('max_pooling2d_2').output)

# Extract the encoder part of the new model
encoder_model_new = Model(inputs=modelnew.input, outputs=modelnew.get_layer('max_pooling2d_2').output)

# Encode the training images using both models
encoded_images = encoder_model.predict(original_ae)
encoded_images_new = encoder_model_new.predict(original_ae)

# Ensure consistency in the size of train_labels
num_samples = encoded_images.shape[0]
train_labels_flatten = train_labels[:num_samples].flatten()

# Visualize the latent spaces with a line plot
plt.figure(figsize=(12, 6))

# Original Model Latent Space
plt.subplot(1, 2, 1)
plt.plot(encoded_images[:, :, :, 0].flatten(), label='Latent Dimension 1', color='red')
plt.plot(encoded_images[:, :, :, 1].flatten(), label='Latent Dimension 2', color='blue')
plt.title('Latent Space - Original Model')
plt.xlabel('Data Point')
plt.ylabel('Latent Dimension Value')
plt.legend()

# New Model Latent Space
plt.subplot(1, 2, 2)
plt.plot(encoded_images_new[:, :, :, 0].flatten(), label='Latent Dimension 1', color='red')
plt.plot(encoded_images_new[:, :, :, 1].flatten(), label='Latent Dimension 2', color='blue')
plt.title('Latent Space - New Model')
plt.xlabel('Data Point')
plt.ylabel('Latent Dimension Value')
plt.legend()

plt.tight_layout()
plt.show()

"""**Latent Spaces in Autoencoder (AE) Model:**

- The original AE model captures the latent representation in a stepwise manner through encoding layers, eventually reducing the input to a latent view layer from 16 to 8. This latent view serves as a compressed representation of the input images, condensing the essential features extracted by the encoding layers.

**Latent Spaces in Variational Autoencoder (VAE) Models:**

- Both VAE and VAE1 models learn latent spaces through a probabilistic approach, capturing the distribution of latent variables for each input sample.

**VAE Latent Space (Dimensionality: 2):**
- VAE with a latent dimension of 2 learns a 2D latent space, allowing for a compact and continuous representation of input images. The encoder produces mean and log variance vectors, which are then used to sample points in the latent space using the reparameterization trick. This 2D space is particularly suitable for visualization, as each point in the latent space corresponds to a generated sample.

**VAE1 Latent Space (Dimensionality: 5):**
- VAE1, with a larger latent dimension of 5, captures a more complex representation in the latent space. The 5D space provides additional capacity for expressing variability in the data. However, it also increases the complexity of the latent space and may require more data to effectively utilize the higher dimensionality.

**Comparison of Latent Spaces:**
- The choice of latent dimensionality influences the expressiveness and capacity of the latent space. While VAE's 2D latent space is simpler, VAE1's 5D latent space can potentially capture more intricate variations in the data.


Overall, the latent spaces learned by the AE, VAE models each serve their respective purposes. The AE models provides a deterministic compressed representation, while VAE models leverage probabilistic latent spaces to capture the inherent uncertainty and variability in the data, with VAE1 having a higher-dimensional space for more intricate representations.

**Performance Discussion**

### Autoencoder (AE) Performance:

1. **Training Stability:**
   - The original AE shows a gradual decrease in loss over 25 epochs, reaching approximately 10.1654.
   - The model appears stable, and the reduction in loss slows down towards the later epochs, suggesting a steady convergence.

2. **Sensitivity to Hyperparameters:**
   - The original AE doesn't explicitly incorporate regularization techniques, and hyperparameters such as learning rate are not explicitly tuned.
   - Stability in training suggests that the chosen hyperparameters might be suitable for this specific task and dataset.

3. **Generated Sample Quality:**
   - The AE aims to denoise grayscale images and uses Mean Squared Error (MSE) as the loss function.
   - The reconstruction quality is assessed by the final training loss, indicating how well the model can reconstruct the input images.

### Variational Autoencoder (VAE) Performance:

1. **Training Stability:**
   - The training history for VAE1 shows a decrease in both reconstruction loss and KL divergence over 25 epochs.
   - VAE2 exhibits similar stability, with a lower starting loss compared to VAE1, indicating a favorable configuration.

2. **Sensitivity to Hyperparameters:**
   - VAE1 explicitly sets the learning rate to 0.001, showcasing the introduction of a specific hyperparameter.
   - The use of a VAE framework, incorporating KL divergence in the loss function, suggests a higher sensitivity to hyperparameters due to the probabilistic nature of the latent space.

3. **Generated Sample Quality:**
   - VAEs are known for generating samples rather than simply reconstructing inputs.
   - The quality is assessed through both the reconstruction loss and KL divergence, ensuring a balance between faithful reconstruction and effective regularization.

### Comparison and Observations:

1. **Stability:**
   - Both AE and VAE models exhibit training stability, with decreasing losses over epochs.
   - The AE model shows stability without explicit regularization techniques, indicating that for simpler tasks, regularization may not be crucial.

2. **Hyperparameter Sensitivity:**
   - VAEs, by nature, are more sensitive to hyperparameters due to their probabilistic framework.
   - Explicit tuning of the learning rate in VAE1 suggests the importance of hyperparameter adjustment for effective training.

3. **Generated Sample Quality:**
   - The AE focuses on denoising and faithfully reconstructing images, while the VAE aims to generate diverse samples by exploring the latent space.
   - VAE2 outperforms VAE1 and AE as you can see the reconstucted images in vae2 model are slightly better than the vae1 and ae models.

Overall, while AE and VAE both exhibit strengths, the VAE's probabilistic nature and sensitivity to hyperparameters make it a powerful framework for tasks demanding generative capabilities and a nuanced understanding of data distribution. The AE, with its simplicity, is effective for more straightforward tasks where complex modeling may not be necessary. The observed differences in performance emphasize the importance of aligning model architectures with specific task requirements and understanding the nuances of hyperparameter tuning in probabilistic frameworks like VAEs.

**References:**
1. https://github.com/fernandolsaraiva/VariationalAutoEncoder-Fashion-MNIST
2. https://github.com/Rahulraj31/Autoencoder_MNIST-Fashion/blob/main/MNIST_Autoencoders.ipynb
"""